{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792d82d6",
   "metadata": {},
   "source": [
    "# Daily News-Driven Stock Market Prediction through Textual Data Analysis and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a292b79",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0064e",
   "metadata": {},
   "source": [
    "# Background of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ffa56",
   "metadata": {},
   "source": [
    "The dataset provided contains news headlines and stock market data, with the goal of predicting if the DJIA Adj Close value will rise or decrease based on the news headlines. The dataset has 27 columns, with the first being the date, the second being the label (1 for rise/stay the same, 0 for decrease), and the rest being the news headlines from \"Top1\" to \"Top25\".\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913c44c",
   "metadata": {},
   "source": [
    "# Problem statement and objective of the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1597e",
   "metadata": {},
   "source": [
    "Problem statement and objective of the project\n",
    "The objective of this project is to build a machine learning model that can predict the rise or decrease of the DJIA Adj Close value based on the news headlines. This is a binary classification task where the model will need to predict one of two labels: \"1\" for rise/stay the same, and \"0\" for decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1c122",
   "metadata": {},
   "source": [
    "# Outline of the methodology and approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96333b",
   "metadata": {},
   "source": [
    "Outline of the methodology and approach\n",
    "The methodology for this project will involve the following steps:\n",
    "\n",
    "Data cleaning and preprocessing\n",
    "Text processing and feature extraction\n",
    "Model selection and training\n",
    "Model evaluation and optimization\n",
    "Conclusion and future work.\n",
    "To tackle the text data, the Natural Language Toolkit (nltk) library in Python will be utilized for text processing and feature extraction. A variety of machine learning algorithms will be trained and evaluated to find the best performing model for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8637f5",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ca7d0",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c515cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b56727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/yassine/Downloads/Combined_News_DJIA.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5cbec",
   "metadata": {},
   "source": [
    "### Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b40a0f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1989, 27)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of the data:\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7a0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head of the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
       "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
       "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
       "      <td>b'Russian tanks are moving towards the capital...</td>\n",
       "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
       "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
       "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
       "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
       "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
       "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
       "      <td>b'This is a busy day:  The European Union has ...</td>\n",
       "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
       "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
       "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
       "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
       "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
       "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
       "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
       "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
       "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
       "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
       "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
       "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
       "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
       "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
       "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
       "      <td>b'China to overtake US as largest manufacturer'</td>\n",
       "      <td>b'War in South Ossetia [PICS]'</td>\n",
       "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
       "      <td>b' Russia has just beaten the United States ov...</td>\n",
       "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
       "      <td>b'Russia is so much better at war'</td>\n",
       "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Remember that adorable 9-year-old who sang a...</td>\n",
       "      <td>b\"Russia 'ends Georgia operation'\"</td>\n",
       "      <td>b'\"If we had no sexual harassment we would hav...</td>\n",
       "      <td>b\"Al-Qa'eda is losing support in Iraq because ...</td>\n",
       "      <td>b'Ceasefire in Georgia: Putin Outmaneuvers the...</td>\n",
       "      <td>b'Why Microsoft and Intel tried to kill the XO...</td>\n",
       "      <td>b'Stratfor: The Russo-Georgian War and the Bal...</td>\n",
       "      <td>b\"I'm Trying to Get a Sense of This Whole Geor...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'U.S. troops still in Georgia (did you know t...</td>\n",
       "      <td>b'Why Russias response to Georgia was right'</td>\n",
       "      <td>b'Gorbachev accuses U.S. of making a \"serious ...</td>\n",
       "      <td>b'Russia, Georgia, and NATO: Cold War Two'</td>\n",
       "      <td>b'Remember that adorable 62-year-old who led y...</td>\n",
       "      <td>b'War in Georgia: The Israeli connection'</td>\n",
       "      <td>b'All signs point to the US encouraging Georgi...</td>\n",
       "      <td>b'Christopher King argues that the US and NATO...</td>\n",
       "      <td>b'America: The New Mexico?'</td>\n",
       "      <td>b\"BBC NEWS | Asia-Pacific | Extinction 'by man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>b' U.S. refuses Israel weapons to attack Iran:...</td>\n",
       "      <td>b\"When the president ordered to attack Tskhinv...</td>\n",
       "      <td>b' Israel clears troops who killed Reuters cam...</td>\n",
       "      <td>b'Britain\\'s policy of being tough on drugs is...</td>\n",
       "      <td>b'Body of 14 year old found in trunk; Latest (...</td>\n",
       "      <td>b'China has moved 10 *million* quake survivors...</td>\n",
       "      <td>b\"Bush announces Operation Get All Up In Russi...</td>\n",
       "      <td>b'Russian forces sink Georgian ships '</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Elephants extinct by 2020?'</td>\n",
       "      <td>b'US humanitarian missions soon in Georgia - i...</td>\n",
       "      <td>b\"Georgia's DDOS came from US sources\"</td>\n",
       "      <td>b'Russian convoy heads into Georgia, violating...</td>\n",
       "      <td>b'Israeli defence minister: US against strike ...</td>\n",
       "      <td>b'Gorbachev: We Had No Choice'</td>\n",
       "      <td>b'Witness: Russian forces head towards Tbilisi...</td>\n",
       "      <td>b' Quarter of Russians blame U.S. for conflict...</td>\n",
       "      <td>b'Georgian president  says US military will ta...</td>\n",
       "      <td>b'2006: Nobel laureate Aleksander Solzhenitsyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>b'All the experts admit that we should legalis...</td>\n",
       "      <td>b'War in South Osetia - 89 pictures made by a ...</td>\n",
       "      <td>b'Swedish wrestler Ara Abrahamian throws away ...</td>\n",
       "      <td>b'Russia exaggerated the death toll in South O...</td>\n",
       "      <td>b'Missile That Killed 9 Inside Pakistan May Ha...</td>\n",
       "      <td>b\"Rushdie Condemns Random House's Refusal to P...</td>\n",
       "      <td>b'Poland and US agree to missle defense deal. ...</td>\n",
       "      <td>b'Will the Russians conquer Tblisi? Bet on it,...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Bank analyst forecast Georgian crisis 2 days...</td>\n",
       "      <td>b\"Georgia confict could set back Russia's US r...</td>\n",
       "      <td>b'War in the Caucasus is as much the product o...</td>\n",
       "      <td>b'\"Non-media\" photos of South Ossetia/Georgia ...</td>\n",
       "      <td>b'Georgian TV reporter shot by Russian sniper ...</td>\n",
       "      <td>b'Saudi Arabia: Mother moves to block child ma...</td>\n",
       "      <td>b'Taliban wages war on humanitarian aid workers'</td>\n",
       "      <td>b'Russia: World  \"can forget about\" Georgia\\'s...</td>\n",
       "      <td>b'Darfur rebels accuse Sudan of mounting major...</td>\n",
       "      <td>b'Philippines : Peace Advocate say Muslims nee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  b\"Georgia 'downs two Russian warplanes' as cou...   \n",
       "1  2008-08-11      1  b'Why wont America and Nato help us? If they w...   \n",
       "2  2008-08-12      0  b'Remember that adorable 9-year-old who sang a...   \n",
       "3  2008-08-13      0  b' U.S. refuses Israel weapons to attack Iran:...   \n",
       "4  2008-08-14      1  b'All the experts admit that we should legalis...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0            b'BREAKING: Musharraf to be impeached.'   \n",
       "1        b'Bush puts foot down on Georgian conflict'   \n",
       "2                 b\"Russia 'ends Georgia operation'\"   \n",
       "3  b\"When the president ordered to attack Tskhinv...   \n",
       "4  b'War in South Osetia - 89 pictures made by a ...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  b'Russia Today: Columns of troops roll into So...   \n",
       "1  b\"Jewish Georgian minister: Thanks to Israeli ...   \n",
       "2  b'\"If we had no sexual harassment we would hav...   \n",
       "3  b' Israel clears troops who killed Reuters cam...   \n",
       "4  b'Swedish wrestler Ara Abrahamian throws away ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  b'Russian tanks are moving towards the capital...   \n",
       "1  b'Georgian army flees in disarray as Russians ...   \n",
       "2  b\"Al-Qa'eda is losing support in Iraq because ...   \n",
       "3  b'Britain\\'s policy of being tough on drugs is...   \n",
       "4  b'Russia exaggerated the death toll in South O...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  b\"Afghan children raped with 'impunity,' U.N. ...   \n",
       "1      b\"Olympic opening ceremony fireworks 'faked'\"   \n",
       "2  b'Ceasefire in Georgia: Putin Outmaneuvers the...   \n",
       "3  b'Body of 14 year old found in trunk; Latest (...   \n",
       "4  b'Missile That Killed 9 Inside Pakistan May Ha...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  b'150 Russian tanks have entered South Ossetia...   \n",
       "1  b'What were the Mossad with fraudulent New Zea...   \n",
       "2  b'Why Microsoft and Intel tried to kill the XO...   \n",
       "3  b'China has moved 10 *million* quake survivors...   \n",
       "4  b\"Rushdie Condemns Random House's Refusal to P...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  b\"Breaking: Georgia invades South Ossetia, Rus...   \n",
       "1  b'Russia angered by Israeli military sale to G...   \n",
       "2  b'Stratfor: The Russo-Georgian War and the Bal...   \n",
       "3  b\"Bush announces Operation Get All Up In Russi...   \n",
       "4  b'Poland and US agree to missle defense deal. ...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  b\"The 'enemy combatent' trials are nothing but...  ...   \n",
       "1  b'An American citizen living in S.Ossetia blam...  ...   \n",
       "2  b\"I'm Trying to Get a Sense of This Whole Geor...  ...   \n",
       "3             b'Russian forces sink Georgian ships '  ...   \n",
       "4  b'Will the Russians conquer Tblisi? Bet on it,...  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0  b'Georgia Invades South Ossetia - if Russia ge...   \n",
       "1  b'Israel and the US behind the Georgian aggres...   \n",
       "2  b'U.S. troops still in Georgia (did you know t...   \n",
       "3                      b'Elephants extinct by 2020?'   \n",
       "4  b'Bank analyst forecast Georgian crisis 2 days...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                b'Al-Qaeda Faces Islamist Backlash'   \n",
       "1  b'\"Do not believe TV, neither Russian nor Geor...   \n",
       "2       b'Why Russias response to Georgia was right'   \n",
       "3  b'US humanitarian missions soon in Georgia - i...   \n",
       "4  b\"Georgia confict could set back Russia's US r...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  b'Condoleezza Rice: \"The US would not act to p...   \n",
       "1  b'Riots are still going on in Montreal (Canada...   \n",
       "2  b'Gorbachev accuses U.S. of making a \"serious ...   \n",
       "3             b\"Georgia's DDOS came from US sources\"   \n",
       "4  b'War in the Caucasus is as much the product o...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  b'This is a busy day:  The European Union has ...   \n",
       "1    b'China to overtake US as largest manufacturer'   \n",
       "2         b'Russia, Georgia, and NATO: Cold War Two'   \n",
       "3  b'Russian convoy heads into Georgia, violating...   \n",
       "4  b'\"Non-media\" photos of South Ossetia/Georgia ...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  b\"Georgia will withdraw 1,000 soldiers from Ir...   \n",
       "1                     b'War in South Ossetia [PICS]'   \n",
       "2  b'Remember that adorable 62-year-old who led y...   \n",
       "3  b'Israeli defence minister: US against strike ...   \n",
       "4  b'Georgian TV reporter shot by Russian sniper ...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  b'Why the Pentagon Thinks Attacking Iran is a ...   \n",
       "1  b'Israeli Physicians Group Condemns State Tort...   \n",
       "2          b'War in Georgia: The Israeli connection'   \n",
       "3                     b'Gorbachev: We Had No Choice'   \n",
       "4  b'Saudi Arabia: Mother moves to block child ma...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  b'Caucasus in crisis: Georgia invades South Os...   \n",
       "1  b' Russia has just beaten the United States ov...   \n",
       "2  b'All signs point to the US encouraging Georgi...   \n",
       "3  b'Witness: Russian forces head towards Tbilisi...   \n",
       "4   b'Taliban wages war on humanitarian aid workers'   \n",
       "\n",
       "                                               Top23  \\\n",
       "0  b'Indian shoe manufactory  - And again in a se...   \n",
       "1  b'Perhaps *the* question about the Georgia - R...   \n",
       "2  b'Christopher King argues that the US and NATO...   \n",
       "3  b' Quarter of Russians blame U.S. for conflict...   \n",
       "4  b'Russia: World  \"can forget about\" Georgia\\'s...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  b'Visitors Suffering from Mental Illnesses Ban...   \n",
       "1                 b'Russia is so much better at war'   \n",
       "2                        b'America: The New Mexico?'   \n",
       "3  b'Georgian president  says US military will ta...   \n",
       "4  b'Darfur rebels accuse Sudan of mounting major...   \n",
       "\n",
       "                                               Top25  \n",
       "0           b\"No Help for Mexico's Kidnapping Surge\"  \n",
       "1  b\"So this is what it's come to: trading sex fo...  \n",
       "2  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...  \n",
       "3  b'2006: Nobel laureate Aleksander Solzhenitsyn...  \n",
       "4  b'Philippines : Peace Advocate say Muslims nee...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nHead of the data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "547e05cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tail of the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>Barclays and RBS shares suspended from trading...</td>\n",
       "      <td>Pope says Church should ask forgiveness from g...</td>\n",
       "      <td>Poland 'shocked' by xenophobic abuse of Poles ...</td>\n",
       "      <td>There will be no second referendum, cabinet ag...</td>\n",
       "      <td>Scotland welcome to join EU, Merkel ally says</td>\n",
       "      <td>Sterling dips below Friday's 31-year low amid ...</td>\n",
       "      <td>No negative news about South African President...</td>\n",
       "      <td>Surge in Hate Crimes in the U.K. Following U.K...</td>\n",
       "      <td>...</td>\n",
       "      <td>German lawyers to probe Erdogan over alleged w...</td>\n",
       "      <td>Boris Johnson says the UK will continue to \"in...</td>\n",
       "      <td>Richard Branson is calling on the UK governmen...</td>\n",
       "      <td>Turkey 'sorry for downing Russian jet'</td>\n",
       "      <td>Edward Snowden lawyer vows new push for pardon...</td>\n",
       "      <td>Brexit opinion poll reveals majority don't wan...</td>\n",
       "      <td>Conservative MP Leave Campaigner: \"The leave c...</td>\n",
       "      <td>Economists predict UK recession, further weake...</td>\n",
       "      <td>New EU 'superstate plan by France, Germany: Cr...</td>\n",
       "      <td>Pakistani clerics declare transgender marriage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>2,500 Scientists To Australia: If You Want To ...</td>\n",
       "      <td>The personal details of 112,000 French police ...</td>\n",
       "      <td>S&amp;amp;P cuts United Kingdom sovereign credit r...</td>\n",
       "      <td>Huge helium deposit found in Africa</td>\n",
       "      <td>CEO of the South African state broadcaster qui...</td>\n",
       "      <td>Brexit cost investors $2 trillion, the worst o...</td>\n",
       "      <td>Hong Kong democracy activists call for return ...</td>\n",
       "      <td>Brexit: Iceland president says UK can join 'tr...</td>\n",
       "      <td>...</td>\n",
       "      <td>US, Canada and Mexico pledge 50% of power from...</td>\n",
       "      <td>There is increasing evidence that Australia is...</td>\n",
       "      <td>Richard Branson, the founder of Virgin Group, ...</td>\n",
       "      <td>37,000-yr-old skull from Borneo reveals surpri...</td>\n",
       "      <td>Palestinians stone Western Wall worshipers; po...</td>\n",
       "      <td>Jean-Claude Juncker asks Farage: Why are you h...</td>\n",
       "      <td>\"Romanians for Remainians\" offering a new home...</td>\n",
       "      <td>Brexit: Gibraltar in talks with Scotland to st...</td>\n",
       "      <td>8 Suicide Bombers Strike Lebanon</td>\n",
       "      <td>Mexico's security forces routinely use 'sexual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>1</td>\n",
       "      <td>Explosion At Airport In Istanbul</td>\n",
       "      <td>Yemeni former president: Terrorism is the offs...</td>\n",
       "      <td>UK must accept freedom of movement to access E...</td>\n",
       "      <td>Devastated: scientists too late to captive bre...</td>\n",
       "      <td>British Labor Party leader Jeremy Corbyn loses...</td>\n",
       "      <td>A Muslim Shop in the UK Was Just Firebombed Wh...</td>\n",
       "      <td>Mexican Authorities Sexually Torture Women in ...</td>\n",
       "      <td>UK shares and pound continue to recover</td>\n",
       "      <td>...</td>\n",
       "      <td>Escape Tunnel, Dug by Hand, Is Found at Holoca...</td>\n",
       "      <td>The land under Beijing is sinking by as much a...</td>\n",
       "      <td>Car bomb and Anti-Islamic attack on Mosque in ...</td>\n",
       "      <td>Emaciated lions in Taiz Zoo are trapped in blo...</td>\n",
       "      <td>Rupert Murdoch describes Brexit as 'wonderful'...</td>\n",
       "      <td>More than 40 killed in Yemen suicide attacks</td>\n",
       "      <td>Google Found Disastrous Symantec and Norton Vu...</td>\n",
       "      <td>Extremist violence on the rise in Germany: Dom...</td>\n",
       "      <td>BBC News: Labour MPs pass Corbyn no-confidence...</td>\n",
       "      <td>Tiny New Zealand town with 'too many jobs' lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
       "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
       "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
       "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
       "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
       "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
       "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
       "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
       "      <td>...</td>\n",
       "      <td>Googles free wifi at Indian railway stations i...</td>\n",
       "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
       "      <td>The men who carried out Tuesday's terror attac...</td>\n",
       "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
       "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
       "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
       "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
       "      <td>We will be swimming in ridicule - French beach...</td>\n",
       "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
       "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "      <td>Brazil: Huge spike in number of police killing...</td>\n",
       "      <td>Austria's highest court annuls presidential el...</td>\n",
       "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
       "      <td>...</td>\n",
       "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
       "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
       "      <td>India gets $1 billion loan from World Bank for...</td>\n",
       "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
       "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
       "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
       "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
       "      <td>Venezuela, where anger over food shortages is ...</td>\n",
       "      <td>A Hindu temple worker has been killed by three...</td>\n",
       "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Label                                               Top1  \\\n",
       "1984  2016-06-27      0  Barclays and RBS shares suspended from trading...   \n",
       "1985  2016-06-28      1  2,500 Scientists To Australia: If You Want To ...   \n",
       "1986  2016-06-29      1                   Explosion At Airport In Istanbul   \n",
       "1987  2016-06-30      1  Jamaica proposes marijuana dispensers for tour...   \n",
       "1988  2016-07-01      1  A 117-year-old woman in Mexico City finally re...   \n",
       "\n",
       "                                                   Top2  \\\n",
       "1984  Pope says Church should ask forgiveness from g...   \n",
       "1985  The personal details of 112,000 French police ...   \n",
       "1986  Yemeni former president: Terrorism is the offs...   \n",
       "1987  Stephen Hawking says pollution and 'stupidity'...   \n",
       "1988   IMF chief backs Athens as permanent Olympic host   \n",
       "\n",
       "                                                   Top3  \\\n",
       "1984  Poland 'shocked' by xenophobic abuse of Poles ...   \n",
       "1985  S&amp;P cuts United Kingdom sovereign credit r...   \n",
       "1986  UK must accept freedom of movement to access E...   \n",
       "1987  Boris Johnson says he will not run for Tory pa...   \n",
       "1988  The president of France says if Brexit won, so...   \n",
       "\n",
       "                                                   Top4  \\\n",
       "1984  There will be no second referendum, cabinet ag...   \n",
       "1985                Huge helium deposit found in Africa   \n",
       "1986  Devastated: scientists too late to captive bre...   \n",
       "1987  Six gay men in Ivory Coast were abused and for...   \n",
       "1988  British Man Who Must Give Police 24 Hours' Not...   \n",
       "\n",
       "                                                   Top5  \\\n",
       "1984      Scotland welcome to join EU, Merkel ally says   \n",
       "1985  CEO of the South African state broadcaster qui...   \n",
       "1986  British Labor Party leader Jeremy Corbyn loses...   \n",
       "1987  Switzerland denies citizenship to Muslim immig...   \n",
       "1988  100+ Nobel laureates urge Greenpeace to stop o...   \n",
       "\n",
       "                                                   Top6  \\\n",
       "1984  Sterling dips below Friday's 31-year low amid ...   \n",
       "1985  Brexit cost investors $2 trillion, the worst o...   \n",
       "1986  A Muslim Shop in the UK Was Just Firebombed Wh...   \n",
       "1987  Palestinian terrorist stabs israeli teen girl ...   \n",
       "1988  Brazil: Huge spike in number of police killing...   \n",
       "\n",
       "                                                   Top7  \\\n",
       "1984  No negative news about South African President...   \n",
       "1985  Hong Kong democracy activists call for return ...   \n",
       "1986  Mexican Authorities Sexually Torture Women in ...   \n",
       "1987  Puerto Rico will default on $1 billion of debt...   \n",
       "1988  Austria's highest court annuls presidential el...   \n",
       "\n",
       "                                                   Top8  ...  \\\n",
       "1984  Surge in Hate Crimes in the U.K. Following U.K...  ...   \n",
       "1985  Brexit: Iceland president says UK can join 'tr...  ...   \n",
       "1986            UK shares and pound continue to recover  ...   \n",
       "1987  Republic of Ireland fans to be awarded medal f...  ...   \n",
       "1988  Facebook wins privacy case, can track any Belg...  ...   \n",
       "\n",
       "                                                  Top16  \\\n",
       "1984  German lawyers to probe Erdogan over alleged w...   \n",
       "1985  US, Canada and Mexico pledge 50% of power from...   \n",
       "1986  Escape Tunnel, Dug by Hand, Is Found at Holoca...   \n",
       "1987  Googles free wifi at Indian railway stations i...   \n",
       "1988  The United States has placed Myanmar, Uzbekist...   \n",
       "\n",
       "                                                  Top17  \\\n",
       "1984  Boris Johnson says the UK will continue to \"in...   \n",
       "1985  There is increasing evidence that Australia is...   \n",
       "1986  The land under Beijing is sinking by as much a...   \n",
       "1987  Mounting evidence suggests 'hobbits' were wipe...   \n",
       "1988  S&amp;P revises European Union credit rating t...   \n",
       "\n",
       "                                                  Top18  \\\n",
       "1984  Richard Branson is calling on the UK governmen...   \n",
       "1985  Richard Branson, the founder of Virgin Group, ...   \n",
       "1986  Car bomb and Anti-Islamic attack on Mosque in ...   \n",
       "1987  The men who carried out Tuesday's terror attac...   \n",
       "1988  India gets $1 billion loan from World Bank for...   \n",
       "\n",
       "                                                  Top19  \\\n",
       "1984             Turkey 'sorry for downing Russian jet'   \n",
       "1985  37,000-yr-old skull from Borneo reveals surpri...   \n",
       "1986  Emaciated lions in Taiz Zoo are trapped in blo...   \n",
       "1987  Calls to suspend Saudi Arabia from UN Human Ri...   \n",
       "1988  U.S. sailors detained by Iran spoke too much u...   \n",
       "\n",
       "                                                  Top20  \\\n",
       "1984  Edward Snowden lawyer vows new push for pardon...   \n",
       "1985  Palestinians stone Western Wall worshipers; po...   \n",
       "1986  Rupert Murdoch describes Brexit as 'wonderful'...   \n",
       "1987  More Than 100 Nobel Laureates Call Out Greenpe...   \n",
       "1988  Mass fish kill in Vietnam solved as Taiwan ste...   \n",
       "\n",
       "                                                  Top21  \\\n",
       "1984  Brexit opinion poll reveals majority don't wan...   \n",
       "1985  Jean-Claude Juncker asks Farage: Why are you h...   \n",
       "1986       More than 40 killed in Yemen suicide attacks   \n",
       "1987  British pedophile sentenced to 85 years in US ...   \n",
       "1988  Philippines president Rodrigo Duterte urges pe...   \n",
       "\n",
       "                                                  Top22  \\\n",
       "1984  Conservative MP Leave Campaigner: \"The leave c...   \n",
       "1985  \"Romanians for Remainians\" offering a new home...   \n",
       "1986  Google Found Disastrous Symantec and Norton Vu...   \n",
       "1987  US permitted 1,200 offshore fracks in Gulf of ...   \n",
       "1988  Spain arrests three Pakistanis accused of prom...   \n",
       "\n",
       "                                                  Top23  \\\n",
       "1984  Economists predict UK recession, further weake...   \n",
       "1985  Brexit: Gibraltar in talks with Scotland to st...   \n",
       "1986  Extremist violence on the rise in Germany: Dom...   \n",
       "1987  We will be swimming in ridicule - French beach...   \n",
       "1988  Venezuela, where anger over food shortages is ...   \n",
       "\n",
       "                                                  Top24  \\\n",
       "1984  New EU 'superstate plan by France, Germany: Cr...   \n",
       "1985                   8 Suicide Bombers Strike Lebanon   \n",
       "1986  BBC News: Labour MPs pass Corbyn no-confidence...   \n",
       "1987  UEFA says no minutes of silence for Istanbul v...   \n",
       "1988  A Hindu temple worker has been killed by three...   \n",
       "\n",
       "                                                  Top25  \n",
       "1984  Pakistani clerics declare transgender marriage...  \n",
       "1985  Mexico's security forces routinely use 'sexual...  \n",
       "1986  Tiny New Zealand town with 'too many jobs' lau...  \n",
       "1987  Law Enforcement Sources: Gun Used in Paris Ter...  \n",
       "1988  Ozone layer hole seems to be healing - US &amp...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTail of the data:\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8df929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1989.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.535445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Label\n",
       "count  1989.000000\n",
       "mean      0.535445\n",
       "std       0.498867\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSummary statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f662c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "Date     0\n",
      "Label    0\n",
      "Top1     0\n",
      "Top2     0\n",
      "Top3     0\n",
      "Top4     0\n",
      "Top5     0\n",
      "Top6     0\n",
      "Top7     0\n",
      "Top8     0\n",
      "Top9     0\n",
      "Top10    0\n",
      "Top11    0\n",
      "Top12    0\n",
      "Top13    0\n",
      "Top14    0\n",
      "Top15    0\n",
      "Top16    0\n",
      "Top17    0\n",
      "Top18    0\n",
      "Top19    0\n",
      "Top20    0\n",
      "Top21    0\n",
      "Top22    0\n",
      "Top23    1\n",
      "Top24    3\n",
      "Top25    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdd8a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# check for missing values again\n",
    "print(df.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5933af",
   "metadata": {},
   "source": [
    "When dealing with missing values, it's important to consider the amount of missing data and how it may impact the results of your analysis. In this case, with only 3 missing values out of 2000 total data points, it's safe to drop the missing values as they are a very small portion of the data and will not significantly impact the results. Dropping the missing values can also simplify the analysis and reduce the risk of introducing bias into the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec692df2",
   "metadata": {},
   "source": [
    "### Distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24fac552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='Label', data=df)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of the target variable')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b11ed8",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235efc3",
   "metadata": {},
   "source": [
    "### A. Tokenizing the text into individual words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ca972",
   "metadata": {},
   "source": [
    "Tokenizing the text into individual words is an important step in text preprocessing. The goal is to convert the text into a format that can be easily analyzed and processed by machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8d48cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yassine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Top1_tokenized  \\\n",
      "0  [b, '', Georgia, 'downs, two, Russian, warplan...   \n",
      "1  [b'Why, wont, America, and, Nato, help, us, ?,...   \n",
      "2  [b'Remember, that, adorable, 9-year-old, who, ...   \n",
      "3  [b, ', U.S., refuses, Israel, weapons, to, att...   \n",
      "4  [b'All, the, experts, admit, that, we, should,...   \n",
      "\n",
      "                                      Top2_tokenized  \\\n",
      "0  [b'BREAKING, :, Musharraf, to, be, impeached, ...   \n",
      "1  [b'Bush, puts, foot, down, on, Georgian, confl...   \n",
      "2  [b, '', Russia, 'ends, Georgia, operation, ', '']   \n",
      "3  [b, '', When, the, president, ordered, to, att...   \n",
      "4  [b'War, in, South, Osetia, -, 89, pictures, ma...   \n",
      "\n",
      "                                      Top3_tokenized  \\\n",
      "0  [b'Russia, Today, :, Columns, of, troops, roll...   \n",
      "1  [b, '', Jewish, Georgian, minister, :, Thanks,...   \n",
      "2  [b, ', '', If, we, had, no, sexual, harassment...   \n",
      "3  [b, ', Israel, clears, troops, who, killed, Re...   \n",
      "4  [b'Swedish, wrestler, Ara, Abrahamian, throws,...   \n",
      "\n",
      "                                      Top4_tokenized  \\\n",
      "0  [b'Russian, tanks, are, moving, towards, the, ...   \n",
      "1  [b'Georgian, army, flees, in, disarray, as, Ru...   \n",
      "2  [b, '', Al-Qa'eda, is, losing, support, in, Ir...   \n",
      "3  [b'Britain\\, 's, policy, of, being, tough, on,...   \n",
      "4  [b'Russia, exaggerated, the, death, toll, in, ...   \n",
      "\n",
      "                                      Top5_tokenized  \\\n",
      "0  [b, '', Afghan, children, raped, with, 'impuni...   \n",
      "1  [b, '', Olympic, opening, ceremony, fireworks,...   \n",
      "2  [b'Ceasefire, in, Georgia, :, Putin, Outmaneuv...   \n",
      "3  [b'Body, of, 14, year, old, found, in, trunk, ...   \n",
      "4  [b'Missile, That, Killed, 9, Inside, Pakistan,...   \n",
      "\n",
      "                                      Top6_tokenized  \\\n",
      "0  [b'150, Russian, tanks, have, entered, South, ...   \n",
      "1  [b'What, were, the, Mossad, with, fraudulent, ...   \n",
      "2  [b'Why, Microsoft, and, Intel, tried, to, kill...   \n",
      "3  [b'China, has, moved, 10, *, million, *, quake...   \n",
      "4  [b, '', Rushdie, Condemns, Random, House, 's, ...   \n",
      "\n",
      "                                      Top7_tokenized  \\\n",
      "0  [b, '', Breaking, :, Georgia, invades, South, ...   \n",
      "1  [b'Russia, angered, by, Israeli, military, sal...   \n",
      "2  [b'Stratfor, :, The, Russo-Georgian, War, and,...   \n",
      "3  [b, '', Bush, announces, Operation, Get, All, ...   \n",
      "4  [b'Poland, and, US, agree, to, missle, defense...   \n",
      "\n",
      "                                      Top8_tokenized  \\\n",
      "0  [b, '', The, 'enemy, combatent, ', trials, are...   \n",
      "1  [b'An, American, citizen, living, in, S.Osseti...   \n",
      "2  [b, '', I, 'm, Trying, to, Get, a, Sense, of, ...   \n",
      "3      [b'Russian, forces, sink, Georgian, ships, ']   \n",
      "4  [b'Will, the, Russians, conquer, Tblisi, ?, Be...   \n",
      "\n",
      "                                      Top9_tokenized  \\\n",
      "0  [b'Georgian, troops, retreat, from, S., Osetta...   \n",
      "1  [b'Welcome, To, World, War, IV, !, Now, In, Hi...   \n",
      "2  [b, '', The, US, military, was, surprised, by,...   \n",
      "3  [b, '', The, commander, of, a, Navy, air, reco...   \n",
      "4  [b'Russia, exaggerating, South, Ossetian, deat...   \n",
      "\n",
      "                                     Top10_tokenized  ...  \\\n",
      "0  [b'Did, the, U.S., Prep, Georgia, for, War, wi...  ...   \n",
      "1  [b, '', Georgia, 's, move, ,, a, mistake, of, ...  ...   \n",
      "2  [b, ', U.S, ., Beats, War, Drum, as, Iran, Dum...  ...   \n",
      "3  [b, '', 92, %, of, CNN, readers, :, Russia, 's...  ...   \n",
      "4  [b, ', Musharraf, expected, to, resign, rather...  ...   \n",
      "\n",
      "                                     Top16_tokenized  \\\n",
      "0  [b'Georgia, Invades, South, Ossetia, -, if, Ru...   \n",
      "1  [b'Israel, and, the, US, behind, the, Georgian...   \n",
      "2  [b, ', U.S, ., troops, still, in, Georgia, (, ...   \n",
      "3             [b'Elephants, extinct, by, 2020, ?, ']   \n",
      "4  [b'Bank, analyst, forecast, Georgian, crisis, ...   \n",
      "\n",
      "                                     Top17_tokenized  \\\n",
      "0         [b'Al-Qaeda, Faces, Islamist, Backlash, ']   \n",
      "1  [b, ', '', Do, not, believe, TV, ,, neither, R...   \n",
      "2  [b'Why, Russias, response, to, Georgia, was, r...   \n",
      "3  [b'US, humanitarian, missions, soon, in, Georg...   \n",
      "4  [b, '', Georgia, confict, could, set, back, Ru...   \n",
      "\n",
      "                                     Top18_tokenized  \\\n",
      "0  [b'Condoleezza, Rice, :, ``, The, US, would, n...   \n",
      "1  [b'Riots, are, still, going, on, in, Montreal,...   \n",
      "2  [b'Gorbachev, accuses, U.S., of, making, a, ``...   \n",
      "3  [b, '', Georgia, 's, DDOS, came, from, US, sou...   \n",
      "4  [b'War, in, the, Caucasus, is, as, much, the, ...   \n",
      "\n",
      "                                     Top19_tokenized  \\\n",
      "0  [b'This, is, a, busy, day, :, The, European, U...   \n",
      "1  [b'China, to, overtake, US, as, largest, manuf...   \n",
      "2  [b'Russia, ,, Georgia, ,, and, NATO, :, Cold, ...   \n",
      "3  [b'Russian, convoy, heads, into, Georgia, ,, v...   \n",
      "4  [b, ', '', Non-media, '', photos, of, South, O...   \n",
      "\n",
      "                                     Top20_tokenized  \\\n",
      "0  [b, '', Georgia, will, withdraw, 1,000, soldie...   \n",
      "1         [b'War, in, South, Ossetia, [, PICS, ], ']   \n",
      "2  [b'Remember, that, adorable, 62-year-old, who,...   \n",
      "3  [b'Israeli, defence, minister, :, US, against,...   \n",
      "4  [b'Georgian, TV, reporter, shot, by, Russian, ...   \n",
      "\n",
      "                                     Top21_tokenized  \\\n",
      "0  [b'Why, the, Pentagon, Thinks, Attacking, Iran...   \n",
      "1  [b'Israeli, Physicians, Group, Condemns, State...   \n",
      "2  [b'War, in, Georgia, :, The, Israeli, connecti...   \n",
      "3           [b'Gorbachev, :, We, Had, No, Choice, ']   \n",
      "4  [b'Saudi, Arabia, :, Mother, moves, to, block,...   \n",
      "\n",
      "                                     Top22_tokenized  \\\n",
      "0  [b'Caucasus, in, crisis, :, Georgia, invades, ...   \n",
      "1  [b, ', Russia, has, just, beaten, the, United,...   \n",
      "2  [b'All, signs, point, to, the, US, encouraging...   \n",
      "3  [b'Witness, :, Russian, forces, head, towards,...   \n",
      "4  [b'Taliban, wages, war, on, humanitarian, aid,...   \n",
      "\n",
      "                                     Top23_tokenized  \\\n",
      "0  [b'Indian, shoe, manufactory, -, And, again, i...   \n",
      "1  [b'Perhaps, *, the, *, question, about, the, G...   \n",
      "2  [b'Christopher, King, argues, that, the, US, a...   \n",
      "3  [b, ', Quarter, of, Russians, blame, U.S., for...   \n",
      "4  [b'Russia, :, World, ``, can, forget, about, '...   \n",
      "\n",
      "                                     Top24_tokenized  \\\n",
      "0  [b'Visitors, Suffering, from, Mental, Illnesse...   \n",
      "1       [b'Russia, is, so, much, better, at, war, ']   \n",
      "2             [b'America, :, The, New, Mexico, ?, ']   \n",
      "3  [b'Georgian, president, says, US, military, wi...   \n",
      "4  [b'Darfur, rebels, accuse, Sudan, of, mounting...   \n",
      "\n",
      "                                     Top25_tokenized  \n",
      "0  [b, '', No, Help, for, Mexico, 's, Kidnapping,...  \n",
      "1  [b, '', So, this, is, what, it, 's, come, to, ...  \n",
      "2  [b, '', BBC, NEWS, |, Asia-Pacific, |, Extinct...  \n",
      "3  [b'2006, :, Nobel, laureate, Aleksander, Solzh...  \n",
      "4  [b'Philippines, :, Peace, Advocate, say, Musli...  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create a list of columns containing text\n",
    "text_columns = [col for col in df.columns if col.startswith('Top')]\n",
    "\n",
    "# tokenize the text in all columns and store the result in a new column with the same name + \"_tokenized\"\n",
    "for col in text_columns:\n",
    "    df[col + '_tokenized'] = df[col].apply(word_tokenize)\n",
    "\n",
    "# show the first 5 rows of the new columns\n",
    "print(df[[col + '_tokenized' for col in text_columns]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05d486",
   "metadata": {},
   "source": [
    "### B. Converting all words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9c60698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all words to lowercase\n",
    "df[text_columns] = df[text_columns].apply(lambda x: x.str.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017776d",
   "metadata": {},
   "source": [
    "df[text_columns] refers to all columns in the dataframe that contain text data.\n",
    "\n",
    ".apply(lambda x: x.str.lower()) applies the lower() method to each element in the selected columns, converting all characters to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae028a",
   "metadata": {},
   "source": [
    "This step is important because, in NLP, it's common to convert all words to lowercase before processing the text data to ensure consistency and reduce the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de21f7e",
   "metadata": {},
   "source": [
    "### C. Stemming or lemmatizing the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e8c99",
   "metadata": {},
   "source": [
    "Lemmatization,  reduces words to their base form using morphological analysis, which results in a meaningful word. For example, the word \"running\" may be reduced to \"run\". Lemmatization is more accurate but slower and more resource-intensive compared to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73598d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[text_columns] = df[text_columns].apply(lambda x: x.apply(lemmatize_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffdf88",
   "metadata": {},
   "source": [
    "### D. Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573dcc7e",
   "metadata": {},
   "source": [
    "The aim of this step is to remove common words that do not carry much meaning, such as \"is\", \"an\", \"the\", etc. These words are called stopwords and are often removed from the text data before further processing. By removing stopwords, we can reduce the dimensionality of the data and focus on the words that carry more meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f1fb490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yassine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yassine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords and punkt corpora from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Create a set of English stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Tokenize the input text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Convert words to lowercase and remove stop words\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Loop through all columns in the dataframe\n",
    "for col in df.columns:\n",
    "    # Check if the column data type is object\n",
    "    if df[col].dtype == 'object':\n",
    "        # Apply the remove_stopwords function to the column if the value is a string\n",
    "        df[col] = df[col].apply(lambda x: remove_stopwords(x) if type(x) == str else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f54900",
   "metadata": {},
   "source": [
    "### E. Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ad3ae",
   "metadata": {},
   "source": [
    "The purpose of this code is to remove all punctuation characters from all string columns in a dataframe. This step is often performed as part of text preprocessing, as punctuation characters can interfere with text analysis and modeling tasks. \n",
    "\n",
    "The code defines a function remove_punctuation that takes a string as input and returns a copy of the string with all punctuation characters removed. The code then loops through all columns in the dataframe and, for each string column, applies the remove_punctuation function to each value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d78307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Define a function to remove punctuation from a string\n",
    "def remove_punctuation(text):\n",
    "    # Create a string of all ASCII punctuation characters\n",
    "    punctuation = string.punctuation\n",
    "    # Use translate method to remove all punctuation characters from the text\n",
    "    text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    return text\n",
    "\n",
    "# Loop through all columns in the dataframe\n",
    "for col in df.columns:\n",
    "    # Check if the column data type is object (e.g. string)\n",
    "    if df[col].dtype == 'object':\n",
    "        # Apply the remove_punctuation function to each string value in the column\n",
    "        df[col] = df[col].apply(lambda x: remove_punctuation(x) if type(x) == str else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e480fdf",
   "metadata": {},
   "source": [
    "### F. Removing non-textual data (e.g. URLs, numbers, special characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7980185",
   "metadata": {},
   "source": [
    "The purpose of removing non-textual data is to clean and pre-process the text data before further analysis. This step helps to eliminate irrelevant and unnecessary information that may not add value to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de4bf87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_textual(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove special characters\n",
    "    return text\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: remove_non_textual(x) if type(x) == str else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76263d34",
   "metadata": {},
   "source": [
    "### H. Removing words that are unique to the corpus or dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22f063",
   "metadata": {},
   "source": [
    "The purpose of removing words that are unique to the corpus or dataset is to eliminate any terms that do not provide meaningful information towards the analysis.\n",
    "\n",
    "These terms are often referred to as \"stop words\" and they are words that occur frequently in the language, but carry little information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "450c1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def remove_corpus_specific_words(text, threshold):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    fdist = FreqDist(words)\n",
    "    stop_words = [word for word, count in fdist.items() if count > threshold]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: remove_corpus_specific_words(x, 100) if type(x) == str else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17382331",
   "metadata": {},
   "source": [
    "This code creates a frequency distribution of the words in the text and eliminates any words that appear more than the specified threshold (in this case, 100 times). \n",
    "\n",
    "The resulting list of words is then joined back into a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d6892",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe38f39",
   "metadata": {},
   "source": [
    "### A. Count-based methods, such as term frequency-inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eda30f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Label','Top1_tokenized', 'Top2_tokenized', 'Top3_tokenized',           'Top4_tokenized', 'Top5_tokenized', 'Top6_tokenized', 'Top7_tokenized',           'Top8_tokenized', 'Top9_tokenized', 'Top10_tokenized',           'Top11_tokenized', 'Top12_tokenized', 'Top13_tokenized',           'Top14_tokenized', 'Top15_tokenized', 'Top16_tokenized',           'Top17_tokenized', 'Top18_tokenized', 'Top19_tokenized',           'Top20_tokenized', 'Top21_tokenized', 'Top22_tokenized',           'Top23_tokenized', 'Top24_tokenized', 'Top25_tokenized']\n",
    "df = df[columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca6cd1",
   "metadata": {},
   "source": [
    "The aim of count-based methods is to convert the text data into numerical features that can be used in machine learning algorithms. One of the most popular count-based methods is TF-IDF (term frequency-inverse document frequency). \n",
    "\n",
    "The idea behind TF-IDF is to weight the words based on how frequent they are in the document, and how rare they are in the entire corpus. This helps to give more importance to the words that are unique to a particular document, and less importance to the words that are common across all documents. \n",
    "\n",
    "The resulting features are the TF-IDF scores for each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86cc3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassine/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_features(dataframe):\n",
    "    tfidf = TfidfVectorizer(min_df=2)  # minimum document frequency to consider a word\n",
    "    features = tfidf.fit_transform(dataframe)\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    return features, feature_names\n",
    "\n",
    "# Apply TF-IDF to all columns with text data\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        text = df[col].apply(str)  # convert pandas series to list of strings\n",
    "        features, feature_names = tfidf_features(text)\n",
    "        df_tfidf = pd.DataFrame(features.todense(), columns=feature_names)\n",
    "        df = pd.concat([df, df_tfidf], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56364fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 84269)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037e924",
   "metadata": {},
   "source": [
    "The number of columns we get depends on the number of unique words in the text data you are transforming. If you have a lot of unique words, you will get a lot of columns in the resulting dataframe. \n",
    "\n",
    "This is normal behavior for the TF-IDF transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada870b1",
   "metadata": {},
   "source": [
    "### B. Word embedding-based methods, such as word2vec or GloVe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f393cc",
   "metadata": {},
   "source": [
    "This code trains a word2vec model on the text data in the 'column_name' column of the dataframe df. The size of the word vectors is set to 100, and the context window size is set to 5. The minimum count of words to be included in the model is set to 1, and 4 worker threads are used for training. \n",
    "\n",
    "The model is saved to a file for future use, and can be loaded again using the gensim.models.Word2Vec.load method.\n",
    "\n",
    "The word vectors can be accessed using the wv attribute of the model, and the vector for a specific word can be accessed using indexing, as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e35abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Top1_tokenized', 'Top2_tokenized', 'Top3_tokenized',           'Top4_tokenized', 'Top5_tokenized', 'Top6_tokenized', 'Top7_tokenized',           'Top8_tokenized', 'Top9_tokenized', 'Top10_tokenized',           'Top11_tokenized', 'Top12_tokenized', 'Top13_tokenized',           'Top14_tokenized', 'Top15_tokenized', 'Top16_tokenized',           'Top17_tokenized', 'Top18_tokenized', 'Top19_tokenized',           'Top20_tokenized', 'Top21_tokenized', 'Top22_tokenized',           'Top23_tokenized', 'Top24_tokenized', 'Top25_tokenized']\n",
    "df = df[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c79a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess your text data\n",
    "text = [text.split() for text in df['Top1_tokenized']]\n",
    "\n",
    "# Train the word2vec model\n",
    "model = Word2Vec(text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model\n",
    "model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Get the word vectors\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Access vector for one word\n",
    "vector = word_vectors['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9649fb5",
   "metadata": {},
   "source": [
    "### C. Character-based methods, such as character n-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c037ac4",
   "metadata": {},
   "source": [
    "In this code, CountVectorizer is used to convert a collection of text documents to a matrix of token counts. The analyzer parameter is set to 'char', which means that the input will be analyzed as a sequence of characters. The ngram_range parameter is set to (2, 4), which means that 2-grams (i.e. bigrams), 3-grams (i.e. trigrams), and 4-grams (i.e. four-grams) will be extracted.\n",
    "\n",
    "The lowercase parameter is set to False, which means that the casing of the characters will be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def char_ngrams_features(dataframe):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 4), lowercase=False)\n",
    "    features = vectorizer.fit_transform(dataframe)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return features, feature_names\n",
    "\n",
    "# Apply character n-grams to all columns with text data\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        features, feature_names = char_ngrams_features(df[col])\n",
    "        df_ngrams = pd.DataFrame(features.todense(), columns=feature_names)\n",
    "        df = pd.concat([df, df_ngrams], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a719bb",
   "metadata": {},
   "source": [
    "### D.  Creating n-grams, which are combinations of words or characters of a specific length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082252fb",
   "metadata": {},
   "source": [
    "n this code, the loop goes through each column in the DataFrame and checks if its data type is np.object, which indicates that it's a textual column.\n",
    "\n",
    "If it is, the code fits the CountVectorizer on the column's values and creates a new DataFrame with the n-grams as columns. Finally, the original DataFrame is concatenated with the n-gram DataFrame to include the n-grams as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a546ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Define the n-gram length\n",
    "n = 3\n",
    "\n",
    "# Initialize a CountVectorizer object\n",
    "vectorizer = CountVectorizer(ngram_range=(n,n))\n",
    "\n",
    "# Loop through each textual column in the DataFrame\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == np.object:\n",
    "        # Fit the CountVectorizer on the column's values\n",
    "        X = vectorizer.fit_transform(df[column].values)\n",
    "        \n",
    "        # Create a new DataFrame with the n-grams as columns\n",
    "        ngrams = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        # Concatenate the original DataFrame with the n-gram DataFrame\n",
    "        df = pd.concat([df, ngrams], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f000108",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d93906aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# extract the text data\n",
    "text_data = df[['Top1_tokenized', 'Top2_tokenized', 'Top3_tokenized',           'Top4_tokenized', 'Top5_tokenized', 'Top6_tokenized', 'Top7_tokenized',           'Top8_tokenized', 'Top9_tokenized', 'Top10_tokenized',           'Top11_tokenized', 'Top12_tokenized', 'Top13_tokenized',           'Top14_tokenized', 'Top15_tokenized', 'Top16_tokenized',           'Top17_tokenized', 'Top18_tokenized', 'Top19_tokenized',           'Top20_tokenized', 'Top21_tokenized', 'Top22_tokenized',           'Top23_tokenized', 'Top24_tokenized', 'Top25_tokenized']]\n",
    "\n",
    "# combine all the text data into a single string for each row\n",
    "text_data = text_data.apply(lambda x: \" \".join(str(x) for x in x), axis=1)\n",
    "\n",
    "# perform count vectorization to convert text into numerical data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# extract the target variable\n",
    "y = df[\"Label\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa724591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd40ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "train_labels = y_train\n",
    "test_data = X_test\n",
    "test_labels = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0824fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassine/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 51.01%\n",
      "Precision: 53.45%\n",
      "Recall: 58.77%\n",
      "F1 Score: 55.98%\n",
      "\n",
      "Model: Support Vector Classifier\n",
      "Accuracy: 52.51%\n",
      "Precision: 52.82%\n",
      "Recall: 97.63%\n",
      "F1 Score: 68.55%\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 52.01%\n",
      "Precision: 53.01%\n",
      "Recall: 83.41%\n",
      "F1 Score: 64.83%\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 53.02%\n",
      "Precision: 54.65%\n",
      "Recall: 66.82%\n",
      "F1 Score: 60.13%\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 47.24%\n",
      "Precision: 50.27%\n",
      "Recall: 43.60%\n",
      "F1 Score: 46.70%\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 50.25%\n",
      "Precision: 53.17%\n",
      "Recall: 51.66%\n",
      "F1 Score: 52.40%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Define the models to be trained\n",
    "models = [LogisticRegression(), SVC(), RandomForestClassifier(), GradientBoostingClassifier(), KNeighborsClassifier(), DecisionTreeClassifier()]\n",
    "model_names = [\"Logistic Regression\", \"Support Vector Classifier\", \"Random Forest\", \"Gradient Boosting\", \"K-Nearest Neighbors\", \"Decision Tree\"]\n",
    "\n",
    "results=[]\n",
    "# Train each model and get predictions\n",
    "for model, model_name in zip(models, model_names):\n",
    "    model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    # Calculate evaluation metrics for each model\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    \n",
    "    # Store the results of each model in the results list\n",
    "    results.append((model_name, accuracy, precision, recall, f1))\n",
    "    \n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"Model: {}\\nAccuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF1 Score: {:.2f}%\\n\".format(result[0], result[1]*100, result[2]*100, result[3]*100, result[4]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d5ab2",
   "metadata": {},
   "source": [
    "Based on the results, none of the models achieved very high performance. It is possible that the models are underfitting or overfitting the data, or the features are not well-suited for the classification task. We shoukd consider exploring alternative feature engineering techniques or trying different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d255c0",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f37bb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   5.6s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   5.3s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   5.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   4.9s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   5.8s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   4.9s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   5.8s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   5.4s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   5.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   5.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   5.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   5.7s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   5.7s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   5.5s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   5.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   5.7s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   5.3s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   5.3s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.9s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.9s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   6.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   4.9s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   5.7s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   5.8s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   5.3s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   5.4s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   5.8s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   5.8s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.9s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.9s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.6s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   5.1s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   5.7s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   5.7s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   5.6s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   4.9s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   5.7s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   5.3s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   5.3s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   5.7s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']}\n",
    "\n",
    "# Initialize the SVC model\n",
    "svc = SVC()\n",
    "\n",
    "# Initialize the GridSearchCV object with the parameter grid and scoring metric\n",
    "grid = GridSearchCV(svc, param_grid, refit=True, verbose=2, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model from the GridSearchCV object\n",
    "best_params = grid.best_params_\n",
    "best_model = grid.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "869269ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, gamma=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c1747e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81b3c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C= 0.1, gamma= 1, kernel= 'rbf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3f60e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels)\n",
    "predictions = model.predict(test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e36d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5301507537688442\n",
      "precision:  0.5301507537688442\n",
      "recall:    1.0\n",
      "f1  :       0.6929392446633826\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate evaluation metrics for each model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "    \n",
    "# Print the results\n",
    "print(f\"accuracy:  {accuracy}\")\n",
    "print(f\"precision:  {precision}\")\n",
    "print(f\"recall:    {recall}\")\n",
    "print(f\"f1  :       {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd97910",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5333d",
   "metadata": {},
   "source": [
    "In this project, several machine learning models were applied to a binary classification problem. The models included Logistic Regression, Support Vector Classifier (SVC), Random Forest, Gradient Boosting, K-Nearest Neighbors, and Decision Tree. The evaluation of these models was performed using various metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Based on the results, the SVC model showed the best performance with an F1 score of 68.55%. The accuracy, precision, and recall were also relatively high compared to the other models. As a result, further tuning of the hyperparameters for the SVC model was performed to improve its performance.\n",
    "\n",
    "The limitations of this project include the limited size of the dataset, which may not be representative of the true underlying distribution of the data. Additionally, only a small number of models were evaluated, and there may be other models that could perform better on this dataset.\n",
    "\n",
    "For future work, it is recommended to collect more data and test additional models to improve the results. Additionally, feature engineering could be performed to extract more relevant information from the data, which may also improve the performance of the models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
